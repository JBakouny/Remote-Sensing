{"cells":[{"cell_type":"markdown","metadata":{"id":"djgJI350TtGf"},"source":["# Section 4: Case Study I - U-Net for Building Mapping\n","\n","Now let's move in to a little advance model call U-Net. U-Net is popular in satellite image analysis (remote sensing) community. Itâ€™s very elegant and simple model that can be used to perform semantic segmentation task (labelling each pixel) well.\n","\n","In this section, we will use U-Net to identify buildings from satellite images (2 class classification problem).\n","\n","    1) U-Net for Building Mapping"]},{"cell_type":"markdown","metadata":{"id":"rAXrj7jOTtGi"},"source":["<hr>\n","<hr>\n","<hr>"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"_PZcw5m6TtGk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679874518225,"user_tz":-180,"elapsed":282,"user":{"displayName":"Laurent Drapeau","userId":"10791757397149067247"}},"outputId":"ef501fc3-70a8-486a-e286-3e9e944e143f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["'''first, let's import libraries '''\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.python.keras import Model\n","from tensorflow.python.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, Concatenate, Dropout\n","import os\n","print(os.getcwd()) "]},{"cell_type":"markdown","metadata":{"id":"HsZr7PpTTtGl"},"source":["## 1) U-Net for Building Mapping\n","\n","Input data are RGB satellite images. And output are binary images. Pixel value is 0 for non-buildings and pixel value is 1 for buildings."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Rsm96y88TtGl","outputId":"c7236eca-f737-4a16-d22f-f6ae0b326446","colab":{"base_uri":"https://localhost:8080/","height":362},"executionInfo":{"status":"error","timestamp":1679874306505,"user_tz":-180,"elapsed":910,"user":{"displayName":"Laurent Drapeau","userId":"10791757397149067247"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-799421085668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# data is already randomized and split in to training / test sets. So we can go ahead and use them as it is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/SpaceNet/sat_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/SpaceNet/bul_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/SpaceNet/sat_test.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/SpaceNet/sat_train.npy'"]}],"source":["'''loading data'''\n","\n","# data is already randomized and split in to training / test sets. So we can go ahead and use them as it is.\n","x_train = np.load('./data/SpaceNet/sat_train.npy').astype('float32')\n","y_train= np.load('./data/SpaceNet/bul_train.npy').astype('float32')\n","x_test = np.load('./data/SpaceNet/sat_test.npy').astype('float32')\n","y_test = np.load('./data/SpaceNet/bul_test.npy').astype('float32') \n","\n","print(\"x_train shape\", x_train.shape)\n","print(\"y_train shape\", y_train.shape)\n","print(\"y_test shape\", x_test.shape)\n","print(\"y_test shape\", y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cfw7RRKYTtGm","executionInfo":{"status":"aborted","timestamp":1679874306507,"user_tz":-180,"elapsed":9,"user":{"displayName":"Laurent Drapeau","userId":"10791757397149067247"}}},"outputs":[],"source":["# Let's plot a sample input RGB image and output image with buildings\n","\n","plt.imshow(x_test[10, :, :, :].astype('uint8'))\n","plt.show()\n","plt.imshow(y_test[10, :, :, 0].astype('uint8'))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"KRh7MtxQTtGn"},"source":["#### First little bit about U-Net\n","\n","Architecture of U-Net is in following figure,\n","\n","<img src=\"https://github.com/LDrap/DL-for-satellite-image-analysis/blob/master/tutorials/graphics/U-Net.PNG?raw=1\" width=\"60%\"/>\n","<sub>Source of the figure: https://arxiv.org/pdf/1505.04597.pdf</sub>\n","\n","It's very elegant and symmetric architecture. U-Net was first developed for biomedical image segmentation in a paper called \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" (https://arxiv.org/pdf/1505.04597.pdf). And later, this architecture was widely adapted in satellite image analysis (remote sensing) community.\n","\n","As in our last examples, in the left side of the network (Encoder), convolutional and max pooling operations down-sample the input image. And in the right side of the network (Decoder), transpose convolutional operations with and without strides up-sample output of the Encoder producing another image with same size as input image as the final output.\n","\n","Another unique thing here is each corresponding down-sampling and up-sampling outputs with same sizes are connected (skip-connections) by __*Concatenation*__ operation. This skip-connections allow gradient (information) to pass through different  levels of the network efficiently, avoiding the bottleneck at the middle of the U-Net architecture. Now a days, these skip connections are widely used in various neural network architectures and they significantly improves performance of neural networks.\n","\n","For skip-connections, we are using inbuilt __*Concatenate*__ function from Tensorflow (Keras) library. Due to this skip-connections, intermediate layers of the U-Net are connected. So U-Net can't be built as a __*Sequential*__ model as in out all past examples. Hence, we are using non-sequential model from Tensorflow (Keras) library for U-Net where input and output of each layer in the neural network is assigned to different variables.\n","\n","Now, let's go ahead and define the U-Net model, fit, prediction and validate the output."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"fjOiqWEYTtGo","executionInfo":{"status":"aborted","timestamp":1679874306507,"user_tz":-180,"elapsed":7,"user":{"displayName":"Laurent Drapeau","userId":"10791757397149067247"}}},"outputs":[],"source":["x_in = Input(shape=(128, 128, 3))\n","\n","'''Encoder'''\n","x_temp = Conv2D(32, (3, 3), activation='relu', padding='same')(x_in)\n","x_temp = Dropout(0.25)(x_temp)\n","x_skip1 = Conv2D(32, (3, 3), activation='relu', padding='same')(x_temp)\n","x_temp = MaxPooling2D((2,2))(x_skip1)\n","x_temp = Conv2D(32, (3, 3), activation='relu', padding='same')(x_temp)\n","x_temp = Dropout(0.25)(x_temp)\n","x_skip2 = Conv2D(32, (3, 3), activation='relu', padding='same')(x_temp)\n","x_temp = MaxPooling2D((2,2))(x_skip2)\n","x_temp = Conv2D(64, (3, 3), activation='relu', padding='same')(x_temp)\n","x_temp = Dropout(0.25)(x_temp)\n","x_skip3 = Conv2D(64, (3, 3), activation='relu', padding='same')(x_temp)\n","x_temp = MaxPooling2D((2,2))(x_skip3)\n","x_temp = Conv2D(64, (3, 3), activation='relu', padding='same')(x_temp)\n","x_temp = Dropout(0.5)(x_temp)\n","x_temp = Conv2D(64, (3, 3), activation='relu', padding='same')(x_temp)\n","\n","'''Decoder'''\n","x_temp = Conv2DTranspose(64, (3, 3), activation='relu',  padding='same')(x_temp)\n","x_temp = Dropout(0.5)(x_temp)\n","x_temp = Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='relu',  padding='same')(x_temp)\n","x_temp = Concatenate()([x_temp, x_skip3])\n","x_temp = Conv2DTranspose(64, (3, 3), activation='relu',  padding='same')(x_temp)\n","x_temp = Dropout(0.5)(x_temp)\n","x_temp = Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='relu',  padding='same')(x_temp)\n","x_temp = Concatenate()([x_temp, x_skip2])\n","x_temp = Conv2DTranspose(32, (3, 3), activation='relu',  padding='same')(x_temp)\n","x_temp = Dropout(0.5)(x_temp)\n","x_temp = Conv2DTranspose(32, (3, 3), strides=(2, 2), activation='relu',  padding='same')(x_temp)\n","x_temp = Concatenate()([x_temp, x_skip1])\n","x_temp = Conv2DTranspose(32, (3, 3), activation='relu',  padding='same')(x_temp)\n","x_temp = Dropout(0.5)(x_temp)\n","x_temp = Conv2DTranspose(32, (3, 3), activation='relu',  padding='same')(x_temp)\n","\n","'''Use 1 by 1 Convolution to get desired output bands'''\n","x_temp = Conv2D(32, (1, 1), activation='relu', padding='same')(x_temp)\n","x_temp = Conv2D(32, (1, 1), activation='relu', padding='same')(x_temp)\n","x_out = Conv2D(1, (1, 1), activation='sigmoid', padding='same')(x_temp)\n","# use sigmoid activation here because output values are either 0 or 1\n","\n","model = Model(inputs=x_in, outputs=x_out)\n","\n","model.compile(loss='mean_squared_error', optimizer='adam')\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"jOOOHhX9TtGq"},"source":["Another new concept here use of multiple __*Dropout*__ layers in the middle of the network. This helps reducing over-fitting during training process. Simply it randomly drops out (ignores) portion of layer outputs (in our case 25%). This allow making robust network architecture finally reducing over-fitting. Short description about __*Dropout*__ can be seen in this nice short video.(https://www.youtube.com/watch?v=NhZVe50QwPM)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"1SL1fdeBTtGr","executionInfo":{"status":"aborted","timestamp":1679874306508,"user_tz":-180,"elapsed":8,"user":{"displayName":"Laurent Drapeau","userId":"10791757397149067247"}}},"outputs":[],"source":["history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=100, batch_size=10, verbose=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8f8Fqnj4TtGs","executionInfo":{"status":"aborted","timestamp":1679874306508,"user_tz":-180,"elapsed":7,"user":{"displayName":"Laurent Drapeau","userId":"10791757397149067247"}}},"outputs":[],"source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fnHL_ZMTtGt","executionInfo":{"status":"aborted","timestamp":1679874306509,"user_tz":-180,"elapsed":8,"user":{"displayName":"Laurent Drapeau","userId":"10791757397149067247"}}},"outputs":[],"source":["'''Prediction over the test dataset'''\n","pred_test = model.predict(x_test)\n","\n","#let's comare random predicted and actial y values \n","plt.imshow(pred_test[20, :, :, 0])\n","plt.show()\n","plt.imshow(y_test[20,:,:,0])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WSDbgVDQTtGu"},"source":["This is not an operational model with high accuracy. But with more layers and with more data, we can develop this architecture in to an operational model with high accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tc5imrT4TtGu","executionInfo":{"status":"aborted","timestamp":1679874306510,"user_tz":-180,"elapsed":9,"user":{"displayName":"Laurent Drapeau","userId":"10791757397149067247"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"colab":{"provenance":[{"file_id":"https://github.com/LDrap/DL-for-satellite-image-analysis/blob/master/tutorials/S4%20Case%20Study%20I%20-%20U-Net%20for%20Building%20Mapping.ipynb","timestamp":1679874264735}]}},"nbformat":4,"nbformat_minor":0}